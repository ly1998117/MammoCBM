# -*- encoding: utf-8 -*-"""@Author :   liuyang@github :   https://github.com/ly1998117/MMCBM@Contact :  liu.yang.mine@gmail.com"""import osimport mpireimport numpy as npimport torchimport redisimport pandas as pdimport pickleimport lightning as Lfrom lightning.pytorch.utilities.types import TRAIN_DATALOADERSfrom monai.transforms import apply_transformfrom tqdm import tqdmfrom copy import deepcopyfrom torch.utils.data import DataLoader, Dataset as _Datasetfrom .transforms import Transformfrom dataset.data_split import DataSplitfrom ast import literal_evalfrom utils import dict_flattentorch.multiprocessing.set_sharing_strategy('file_system')class ImagesReader:    def __init__(self, pathology_labels, transform):        self.pathology_labels = pathology_labels        self.transform = transform    def __call__(self, data_i: dict, pathology=None, name=None):        """        Fetch single dataset item.        data_i: {'FA': [path1, path2, path3], 'ICGA': [path1, path2, path3], 'US': [path1, path2, path3],        'label': 0, 'pathology': 0, 'name': 0}        """        data_i = {m: [i for i in v if i is not None] for m, v in data_i.items() if len(v) > 0}        data_i = {m: v for m, v in data_i.items() if len(v) > 0}        data_i = dict(data=data_i, pathology=[pathology], name=[name], img={m: v for m, v in data_i.items()},                      modality=['MM'] if len(data_i) > 1 and 'US' in list(data_i.keys()) else list(data_i.keys()),                      label=[None])        if self.transform is not None:            data_i['dataset']: dict = apply_transform(self.transform, data_i['dataset'])        data_i['dataset'] = {m: torch.stack(d, dim=0).unsqueeze(0) if isinstance(d, list) else d.unsqueeze(0)                             for m, d in data_i['dataset'].items()}        if pathology is not None:            data_i['label'] = torch.tensor(self.pathology_labels[pathology], dtype=torch.long)        return data_iclass Dataset(_Dataset):    def __init__(self, data: pd.DataFrame, transform, pathology_labels, n_shot='full', redis_client=None,                 include_concept=False):        super(Dataset, self).__init__()        self.transform = transform        self.pathology_labels = pathology_labels        self.n_shot = n_shot        self.data = self.prepare_data(data)        self.redis_client = redis_client        self.patient = None        self.include_concept = include_concept        if len(self.data) > 0:            print(f'modality: {self.data[0]["modality"]}')    def prepare_data(self, data):        if data is None:            return []        df = data.reset_index(drop=True)        if self.n_shot != 'full':            df = df.sample(n=self.n_shot, random_state=42)        df['label'] = df['pathology'].map(lambda data_i: self.pathology_labels[data_i])        df['img'] = df['path'].map(lambda data_i: {m: p.split('/')[-1] for m, p in data_i.items() if isinstance(p, str)})        # df['img'] = df['name']        df['data'] = df['path'].map(lambda x: {k: v for k, v in x.items()})        return df.to_dict('records')    def __len__(self):        return len(self.data)    def get_item_from_redis(self, pid, data):        for m, p in data.items():            key = f'{pid}_{m}'            if not self.redis_client.exists(key):                raise KeyError(f'key {key} not found in redis')            img = self.redis_client.get(key)            data[m] = pickle.loads(img)        return data    def __getitem__(self, index: int):        """        Fetch single dataset item from `self.dataset`.        """        data_i = deepcopy(self.data[index])        if self.redis_client is not None:            data_i['data'] = self.get_item_from_redis(data_i['pid'], data_i['data'])        if self.transform is not None:            data_i['data']: dict = apply_transform(self.transform, data_i['data'])        for m in data_i['data'].keys():            if isinstance(data_i['data'][m], list):                if isinstance(data_i['data'][m][0], torch.Tensor):                    data_i['data'][m] = torch.stack(data_i['data'][m], dim=0)                else:                    data_i['data'][m] = torch.tensor(data_i['data'][m], dtype=torch.float)        out = {'name': data_i['name'], 'pathology': data_i['pathology'], 'modality': data_i['modality'],               'img': data_i['img'], 'data': data_i['data'], 'label': data_i['label']}        if self.include_concept:            concept_label = [cl for ck, cl in zip(data_i['concept_key'], data_i['concept_label']) if                             'BI-RADS_category' not in ck]            assert len(concept_label) == 47            out['concept_label'] = np.array(concept_label, dtype=float)        return outclass FeatureDataset(Dataset):    def __init__(self, encoder, data_path, stage, fold, data: pd.DataFrame, transform, pathology_labels, n_shot='full',                 redis_client=None, include_concept=False, encoder_name=None):        super().__init__(data, transform, pathology_labels, n_shot, redis_client, include_concept=include_concept)        self.encoder = encoder        if len(self) == 0:            return        if self.encoder is not None:            if encoder_name is None:                encoder_name = encoder.name            self.modality = self.data[0]['modality']            self.stage = stage            self.fold = fold            self.cache_path = (f'{data_path}/Cache/{encoder_name}-{self.modality}-{self.stage}-'                               f'Pooling{self.encoder.avg_pooling}-Fold{self.fold}.pth')            self.cache()    def encode_image(self, data, modality):        data = {k: v.unsqueeze(0) for k, v in data.items()}        data = self.encoder.encode_image(data, modality=modality, fusion=False)        data = {k: v.cpu()[0] for k, v in data.items()}        if modality in data:            data = data[modality]        return data    @staticmethod    def extract_item(i, data_i, transform):        data_i = deepcopy(data_i)        if transform is not None:            data_i['data']: dict = apply_transform(transform, data_i['data'])        for m in data_i['data'].keys():            if isinstance(data_i['data'][m], list):                if isinstance(data_i['data'][m][0], torch.Tensor):                    data_i['data'][m] = torch.stack(data_i['data'][m], dim=0)                else:                    data_i['data'][m] = torch.tensor(data_i['data'][m], dtype=torch.float)        return i, data_i['data'], data_i['modality']    @torch.no_grad()    def cache(self):        if not os.path.exists(self.cache_path):            os.makedirs(os.path.dirname(self.cache_path), exist_ok=True)            # dataset = FnDataset(self.data, super(FeatureDataset, self).__getitem__)            # for i in tqdm(range(len(self.data)), desc=f'Caching Embeddings to {self.cache_path}'):            #     d = super(FeatureDataset, self).__getitem__(i)            #     self.data[i]['data'] = self.encode_image(d.pop('data'), d['modality'])            print(f'{self.__class__.__name__} Caching Embeddings to {self.cache_path}')            with mpire.WorkerPool(n_jobs=16, start_method='spawn') as pool:                results: list = pool.map(FeatureDataset.extract_item,                                         [(i, self.data[i], self.transform) for i in range(len(self.data))],                                         progress_bar=True)            for result in tqdm(results, desc=f'Caching Embeddings to {self.cache_path}'):                i, d, m = result                self.data[i]['data'] = self.encode_image(d, m)            torch.save(self.data, self.cache_path)        else:            self.data = torch.load(self.cache_path, weights_only=False, map_location='cpu')    def get_preprocessed_img(self, index):        return super().__getitem__(index)    def __getitem__(self, index: int):        if self.encoder is None:            return self.get_preprocessed_img(index)        data_i = self.data[index]        out = {'name': data_i['name'], 'pathology': data_i['pathology'], 'modality': data_i['modality'],               'img': data_i['img'], 'data': data_i['data'], 'label': data_i['label']}        if self.include_concept:            concept_label = [cl for ck, cl in zip(data_i['concept_key'], data_i['concept_label']) if                             'BI-RADS_category' not in ck]            out['concept_label'] = np.array(concept_label, dtype=float)        return outclass ConceptDataset(FeatureDataset):    def __init__(self, encoder, data_path, fold, transform, pathology_labels, n_shot='full', encoder_name=None):        if encoder_name is None:            encoder_name = encoder.name        # cpath = os.path.join(data_path, 'CSV', 'report', 'concepts.csv')        # if os.path.exists(cpath):            # data = pd.read_csv(cpath)            # data['concept'] = data['concept'].map(literal_eval).map(self.bool_2_label).map(dict_flatten)        # else:        data = pd.read_csv(os.path.join(data_path, 'CSV', 'data_split', 'k_fold.csv'))        data['concept_label'] = data['concept_label'].map(literal_eval)        data['concept_key'] = data['concept_key'].map(literal_eval)        data['concept'] = data.apply(lambda x: dict(zip(x['concept_key'], x['concept_label'])), axis=1)        if 'bbox' in data.columns and not data['bbox'].hasnans:            data['bbox'] = data['bbox'].map(literal_eval).map(dict_flatten)        data['path'] = data['path'].map(literal_eval)        super(FeatureDataset, self).__init__(data=data, transform=transform, pathology_labels=pathology_labels,                                             n_shot=n_shot, redis_client=None,                                             include_concept=False)        self.encoder = encoder        self.modality = self.data[0]['modality']        self.fold = fold        self.cache_path = f'{data_path}/Cache/ConceptDataset-{encoder_name}-{self.modality}-Fold{fold}.pth'        self.cache()    def encode_image(self, data, modality):        data = {k: v.unsqueeze(0) for k, v in data.items()}        try:            data = self.encoder.encode_image(data, modality=modality, fusion=True, avg_pooling=True)        except:            import pdb; pdb.set_trace()        data = {k: v.cpu()[0] for k, v in data.items()}        if modality in data:            data = data[modality]        return data    def bool_2_label(self, d):        if isinstance(d, dict):            return {k: self.bool_2_label(v) for k, v in d.items()}        if isinstance(d, list):            return [self.bool_2_label(l) for l in d]        if isinstance(d, bool):            return int(d)        return d    def __getitem__(self, item):        data_i = self.data[item]        concept = data_i['concept']        concept = {k: v for k, v in concept.items() if 'BI-RADS_category' not in k}        if 'bbox' in data_i.keys():            return dict(concept=concept, bbox=data_i['bbox'], data=data_i['data'])        if 'k' in data_i.keys():            return dict(concept=concept, data=data_i['data'], fold=data_i['k'])        return dict(concept=concept, data=data_i['data'])class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler[int]):    """Samples elements randomly from a given list of indices for imbalanced dataset    Arguments:        indices (list, optional): a list of indices        num_samples (int, optional): number of samples to draw    """    def __init__(self, dataset, indices=None):        # if indices is not provided,        # all elements in the dataset will be considered        super().__init__(dataset)        self.dataset = dataset        self.indices = list(range(len(dataset))) \            if indices is None else indices        # if num_samples is not provided,        # draw `len(indices)` samples in each iteration        self.num_samples = len(self.indices)        # distribution of classes in the dataset        label_to_count = {}        for idx in self.indices:            label = self._get_label(idx)            if label not in label_to_count:                label_to_count.update({label: 0})            label_to_count[label] += 1        # weight for each sample        weights = [1.0 / label_to_count[self._get_label(idx)]                   for idx in self.indices]        self.weights = torch.DoubleTensor(weights)    def _get_label(self, idx):  # Note: for single attribute dataset        return self.dataset.data[idx]['label']  # [0]    def __iter__(self):        idx = (self.indices[i] for i in torch.multinomial(            self.weights, self.num_samples, replacement=True))        return idx    def __len__(self):        return self.num_samplesclass MMDataLoader:    def __init__(self, loaders, stage, modality):        super().__init__()        self.loaders = loaders        self.modality = modality        self.stage = stage        self.batch_size = loaders[0].batch_size        self.num_workers = loaders[0].num_workers        self.loader = None        # 在 __iter__ 中初始化 iter_loaders 和 index        self.iter_loaders = []        self.current_loader_index = 0        self.length = sum([len(loader) for loader in self.loaders])    def __len__(self):        return self.length    def empty(self):        return len(self.loaders) == 0    def get_loader(self):        if len(self.iter_loaders) == 0:            raise StopIteration        loader = self.iter_loaders[self.current_loader_index]        return loader    def remove_loader(self):        self.iter_loaders.pop(self.current_loader_index)        if len(self.iter_loaders) == 0:            raise StopIteration        self.current_loader_index %= len(self.iter_loaders)    def __iter__(self):        self.iter_loaders = [loader.__iter__() for loader in self.loaders]        self.current_loader_index = 0        return self    def __next__(self):        loader = self.get_loader()        try:            batch = next(loader)            self.current_loader_index = (self.current_loader_index + 1) % len(self.iter_loaders)            # if 'MM' not in self.modality:            #     # 单模态情况            #     batch['modality'] = [self.modality] * self.batch_size            # elif batch['modality'][0] == 'MM':            #     batch['modality'] = [self.modality] * self.batch_size            return batch        except StopIteration:            self.remove_loader()            return self.__next__()class DataModule(L.LightningDataModule):    def __init__(self, config, transform=None, feature=False, encoder=None):        super().__init__()        self.datasets = None        self.config = config        self.dataSpliter = DataSplit(data_path=f'./dataset/{config.dataset}',                                     valid_only=config.valid_only,                                     test_only=config.test_only,                                     same_valid=config.same_valid,                                     test_csv=config.test_csv,                                     under_sample=config.under_sample)        self.transform = Transform(root_dir='./dataset',                                   normalize=config.z_normalize,                                   img_size=config.img_size,                                   spacing=config.spacing,                                   crop_prob=config.crop_prob) if transform is None else {            'train': transform, 'val': transform, 'test': transform        }        if config.use_redis:            self.redis_client = redis.Redis(host='localhost', port=6379, db=0)            # self.redis_client.flushdb()        else:            self.redis_client = None        self.feature = feature        self.encoder = encoder        self.include_concept = config.include_concept    def to(self, x, device):        if isinstance(x, torch.Tensor):            x = x.to(device)        if isinstance(x, (list, tuple)):            x = [self.to(_x, device) for _x in x]        if isinstance(x, dict):            x = {k: self.to(v, device) for k, v in x.items()}        return x    def prepare_data(self) -> None:        if self.redis_client is not None:            reader = self.transform.image_reader()            data_df = self.dataSpliter.df[['pid', 'path']]            for i, row in tqdm(data_df.iterrows(), total=len(data_df), desc='Caching images to redis'):                paths = row['path']                pid = row['pid']                for modality, path in paths.items():                    key = f'{pid}_{modality}'                    lock_key = f'{key}_lock'                    lock = self.redis_client.lock(lock_key, timeout=60)  # force release lock after 60 seconds                    if not lock.acquire(blocking=False):                        continue                    try:                        if self.redis_client.exists(key):                            continue                        if modality != 'TIC':                            img = reader.operator(modality, path)                        else:                            img = literal_eval(path) if isinstance(path, str) and '[' in path else path                        img = pickle.dumps(img)                        self.redis_client.set(key, img)                    finally:                        lock.release()        self.datasets = {            'train': self.get_datasets('train'),            'val': self.get_datasets('val'),            'test': self.get_datasets('test'),        }    def get_datasets(self, stage):        if self.datasets and stage in self.datasets:            return self.datasets[stage]        datasets = []        # for modality in self.config.modality_to_dataset[self.config.modality]:        dataset = FeatureDataset(            encoder=self.encoder,            data_path=f'dataset/{self.config.dataset}',            stage=stage,            fold=self.config.k,            data=self.dataSpliter.get_data_split(                modality='MM',                k=self.config.k,            )[stage],            transform=self.transform[stage] if self.encoder is None else self.transform['val'],            n_shot=self.config.n_shot,            pathology_labels=self.config.pathology_labels,            redis_client=self.redis_client,            include_concept=self.include_concept,            encoder_name=self.config.encode_dir.replace('/', '-') + self.config.postfix if hasattr(self.config,                                                                                                   'encode_dir') else None        )        if len(dataset) > 0:            datasets.append(dataset)        return datasets    def get_dataloader(self, stage):        loaders = [DataLoader(            dataset=dataset,            sampler=ImbalancedDatasetSampler(dataset) if self.config.imbalance else None,            batch_size=self.config.batch_size,            shuffle=stage == 'train',            num_workers=self.config.num_worker,            pin_memory=False,            persistent_workers=False        ) for dataset in self.datasets[stage]]        if len(loaders) == 0:            return None        return MMDataLoader(loaders=loaders, stage=stage, modality=self.config.modality)    @torch.no_grad()    def setup(self, stage: str) -> None:        pass    def train_dataloader(self) -> TRAIN_DATALOADERS:        return self.get_dataloader('train')    def val_dataloader(self) -> TRAIN_DATALOADERS:        val = self.get_dataloader('val')        if val is None:            val = self.get_dataloader('test')        return val    def test_dataloader(self) -> TRAIN_DATALOADERS:        return self.get_dataloader('test')